{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1ea91c-3a97-4bd5-af5b-c0c674d13748",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Instalacion de Requisitos y Creación de Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351a119f-ccb7-4c6a-b759-d1f0c27080fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5617797-3616-47f3-b2a8-00034212446e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializa una sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Regresion-Berka\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04687477-ad46-4c61-bb6e-dfdeeed41a43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ingesta y Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366c1a2d-4e52-4a1b-9328-fb09b516484f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "consulta_sql = \"\"\"\n",
    "SELECT *\n",
    "FROM view_berka\n",
    "\"\"\"\n",
    "data = spark.sql(consulta_sql)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8847911-bd8b-487f-afd7-c67fe8e9905e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocesamiento de datos (limpieza, conversión de categorías, etc.)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# Renonbrar columnas\n",
    "data = data.withColumnRenamed('A11','avg_salary')\n",
    "data = data.withColumnRenamed('A1','district_id')\n",
    "data = data.withColumnRenamed('edad','client_age')\n",
    "\n",
    "# Crear un StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"status\", outputCol=\"status_index\")\n",
    "\n",
    "# Convierte las columnas de cadena a tipo de dato numérico\n",
    "data = data.withColumn(\"amount\", data[\"amount\"].cast(\"double\"))\n",
    "data = data.withColumn(\"payments\", data[\"payments\"].cast(\"double\"))\n",
    "data = data.withColumn(\"duration\", data[\"duration\"].cast(\"double\"))\n",
    "data = data.withColumn(\"avg_salary\", data[\"avg_salary\"].cast(\"double\"))\n",
    "\n",
    "# Ajustar el StringIndexer al DataFrame y transformar los datos\n",
    "df_berka_final = indexer.fit(data).transform(data)\n",
    "\n",
    "# Mostrar el resultado\n",
    "df_berka_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb369046-85bf-40f0-a850-74baef17f2c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  -> Realizar un agrupamiento de las variables cualitativas que usara (tabla de frecuencia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d673d4b2-36a4-4da2-8da3-f51ec356efb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Calcula la frecuencia de cada valor único en la columna \"status\"\n",
    "status_frequencies = df_berka_final.groupBy(\"status\").agg(count(\"*\").alias(\"frequency\"))\n",
    "# Muestra la tabla de frecuencias\n",
    "status_frequencies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018b1571-1e49-438a-aa82-47d2c23ed6dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### -> Para el caso de una variable cuantitativa usar el método describe para obtener estadísticos descriptivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa6eb98-b0e6-4398-8b0c-fb6ca9fef3ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecciona las columnas cuantitativas que deseas analizar.\n",
    "cuantitativas = [\"payments\", \"duration\", \"amount\", \"avg_salary\", \"client_age\"]\n",
    "\n",
    "# Usa el método \"describe\" en estas columnas.\n",
    "describe_stats = df_berka_final.select(*cuantitativas).describe()\n",
    "\n",
    "# Muestra las estadísticas descriptivas.\n",
    "describe_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e501ab52-f55b-42b2-a6d1-4f9cee270bab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24335d1-b6b1-4661-b36e-eb33e7d9542f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# División de datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = df_berka_final.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e555ded-beb6-46ae-a7b7-618fc072759b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creación de un DataFrame de características\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\"amount\", \"payments\", \"duration\", \"client_age\", \"avg_salary\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a454d15-5208-4c3e-ad69-c9b3794a7eda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### creacion de datos especiales para MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8741aa32-9430-4807-b561-bde53aff4094",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar esta linea en caso se quiera volver a ejecutar el paso 4\n",
    "#train_data = train_data.drop(\"features\")\n",
    "#test_data = test_data.drop(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926a44be-57ac-4fa1-b21a-169ff6c6f7fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "ml_data = assembler.transform(df_berka_final)\n",
    "ml_data = ml_data.withColumnRenamed(\"status_index\", \"label\")\n",
    "ml_data = ml_data.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "\n",
    "# División de datos en conjuntos de entrenamiento y prueba\n",
    "(ml_train_data, ml_test_data) = ml_data.randomSplit([0.8, 0.2], seed=1234)\n",
    "ml_test_data = ml_test_data.withColumn(\"label\", col(\"label\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2937376e-694f-42d8-b929-26d27ad9437b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8fa9bf-c697-43a7-a0d6-8e2ed4d1ccc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Modelo LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd352a4-713a-47b6-a743-e95ac23c9d80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"status_index\")\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "lr_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1bfbe41-6ac1-4799-b181-89114f438269",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Modelo RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4bfcaae-c65f-45db-81c5-a48bf54691da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(numTrees=45, labelCol=\"status_index\", featuresCol=\"features\")\n",
    "rf_model = rf_classifier.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd59adc3-efef-4d69-8286-9af7960b1876",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Modelo MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9ae995-adcf-4279-8263-7f4367429126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "ml_layers = [len(feature_cols), 10, 5, len(ml_data.select(\"label\").distinct().collect())]\n",
    "ml_classifier = MultilayerPerceptronClassifier(maxIter=120, layers=ml_layers, blockSize=256, seed=1234)\n",
    "ml_model = ml_classifier.fit(ml_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f3e39d-2e72-4964-8b59-87bae45da182",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Evaluación de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49250f0-0f8a-42a1-828e-5a5931434e18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Crear un diccionario de modelos y etiquetas de columna\n",
    "models_and_labels = {\n",
    "    \"LogisticRegression\": (\"status_index\", lr_model),\n",
    "    \"RandomForest\": (\"status_index\", rf_model),\n",
    "    \"MultilayerPerceptron\": (\"label\", ml_model)\n",
    "}\n",
    "\n",
    "# Crear un diccionario de métricas\n",
    "metrics = {\n",
    "    \"F1 Score\": \"f1\",\n",
    "    \"Precision\": \"weightedPrecision\",\n",
    "    \"Recall\": \"weightedRecall\",\n",
    "    \"Weighted F1 Score\": \"weightedFMeasure\",\n",
    "    \"Accuracy\": \"accuracy\"\n",
    "}\n",
    "\n",
    "# Realizar predicciones y calcular varias métricas para cada modelo\n",
    "for model_name, (label_col, model) in models_and_labels.items():\n",
    "    predictions = model.transform(test_data if model_name != \"MultilayerPerceptron\" else ml_test_data)\n",
    "    \n",
    "    print(f\"Resultados para el modelo {model_name}:\")\n",
    "    \n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=metric_value)\n",
    "        metric_result = evaluator.evaluate(predictions)\n",
    "        print(f\"{metric_name}: {metric_result}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f89a2d-2f60-473d-a658-c2226c9e325e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Probando hiperparametros en MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88047bb-3d3a-49a6-b3fc-d2dcd88bf888",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Definir una cuadrícula de hiperparámetros a explorar\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(ml_classifier.maxIter, [100, 200])\n",
    "              .addGrid(ml_classifier.layers, [[5, 10, 4], [5, 10, 10, 4]])\n",
    "              .build())\n",
    "\n",
    "# Crear un evaluador\n",
    "cross_ml_evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "# Configurar la validación cruzada\n",
    "ml_crossval = CrossValidator(estimator=ml_classifier,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=cross_ml_evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Ejecutar la validación cruzada para encontrar los mejores hiperparámetros\n",
    "cv_model = ml_crossval.fit(ml_train_data)\n",
    "best_model = cv_model.bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44eb3d3a-6127-48de-be7b-af6a3849c485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo en los datos de prueba (o cualquier otro conjunto de datos)\n",
    "ml_test_predictions = best_model.transform(ml_test_data)\n",
    "\n",
    "# Crear un evaluador para calcular métricas\n",
    "cross_ml_evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "# Calcular el F1 Score en los datos de prueba\n",
    "cross_ml_f1_score = cross_ml_evaluator.evaluate(ml_test_predictions)\n",
    "\n",
    "# Imprimir el F1 Score y otras métricas si es necesario\n",
    "print(f\"F1 Score: {cross_ml_f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Regresion_Berka",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
