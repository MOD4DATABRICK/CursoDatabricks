{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d00545e-7625-49ed-ad34-639bb8235d01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Instalacion de Requisitos y Creación de Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3972fe09-d4b4-47ec-b362-fa99dbd1667c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfff623e-f675-4d52-addc-a7efd7f9137b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import CrossValidatorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea86bd18-590c-44bb-899c-4b8f9a3d5d2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"KMeans-Caso-Real\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b513c2de-0774-4ff7-8d04-9fde52c10f46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ingesta y Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0908054e-2b26-475c-af6f-2c9e3d3835de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carga de datos desde un archivo CSV\n",
    "data = spark.read.option(\"header\", \"true\").csv(\"/FileStore/practica-3/view_caso_real.csv\")\n",
    "data.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30335317-9c6e-428a-9982-41a7dcb69b63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correccion de nombres en la columnas.\n",
    "data = data.withColumnRenamed('Cuontas_Restantes','Cuotas_Restantes')\n",
    "\n",
    "# Rellenar valores nulos en la columna \"Aclaracion\" con un valor predeterminado (por ejemplo, \"Sin Aclaracion\")\n",
    "data = data.withColumn(\"Aclaracion\", when(data[\"Aclaracion\"].isNull(), \"Sin Aclaracion\").otherwise(data[\"Aclaracion\"]))\n",
    "\n",
    "# Aplicar el StringIndexer al DataFrame con valores predeterminados\n",
    "aclaracionIndexer = StringIndexer(inputCol=\"Aclaracion\", outputCol=\"Aclaracion_index\")\n",
    "df_clean = aclaracionIndexer.fit(data).transform(data)\n",
    "\n",
    "# Convertir las columnas seleccionadas de string a int\n",
    "featuresCols = [\"Monto_Moneda_Local\", \"Monto_Moneda_Extranjera\", \n",
    "                \"Cuotas_Pagadas\", \"Cuotas_Devengadas\", \n",
    "                \"Cuotas_Restantes\", \"Aclaracion_index\"]\n",
    "for feature in featuresCols:\n",
    "    df_clean = df_clean.withColumn(feature, col(feature).cast(\"int\"))\n",
    "    \n",
    "df_clean.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852f9df7-28c7-4c94-9ad8-6347576223b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be06438-87cf-4a5a-82cb-a980ccd54759",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=featuresCols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True, withStd=True)\n",
    "\n",
    "# Paso 4: División de datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = df_clean.randomSplit([0.7, 0.3], seed=42)  # 70% para entrenamiento, 30% para prueba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7f2649-23e8-4d26-9273-72579230c130",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creación y Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20291256-5741-4596-8f50-bc0bd87a100c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 5: Definir el modelo K-Means\n",
    "kmeans = KMeans().setSeed(1).setFeaturesCol(\"scaledFeatures\").setPredictionCol(\"cluster\")\n",
    "\n",
    "# Paso 6: Crear un Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "# Paso 7: Crear una cuadrícula de parámetros para búsqueda en cuadrícula\n",
    "param_grid = ParamGridBuilder().addGrid(kmeans.k, [3, 4, 5, 6, 7, 8, 2])\\\n",
    ".addGrid(kmeans.initMode, [\"k-means||\", \"random\"])\\\n",
    ".addGrid(kmeans.maxIter, [10, 20, 30, 40,45,50])\\\n",
    ".build()\n",
    "\n",
    "# Paso 8: Configurar el evaluador\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\", featuresCol=\"features\")\n",
    "\n",
    "# Paso 9: Configurar la validación cruzada\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Paso 10: Ajustar el modelo con búsqueda en cuadrícula\n",
    "cv_model = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e253b0f-8ee1-4014-b1ce-09071e2ffebd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2879ac13-5f3e-4101-aaae-682a0af449ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener el mejor modelo\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Aplicar el mejor modelo a los datos de prueba\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Mostrar la configuracion de hiperparámetros del mejor modelo\n",
    "print(f\"Mejor valor de K: {best_model.stages[-1].getK()}\")\n",
    "print(f\"Mejor valor de initMode: {best_model.stages[-1].getInitMode()}\")\n",
    "print(f\"Mejor valor de maxIter: {best_model.stages[-1].getMaxIter()}\")\n",
    "\n",
    "# Evaluacion del modelo utilizando Silhouette\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\", featuresCol=\"features\", metricName=\"silhouette\")\n",
    "silhouette_score = evaluator.evaluate(predictions)\n",
    "\n",
    "\n",
    "print(f\"El Silhouette Score es: {silhouette_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d96420-706c-418f-9fe4-1f9d10ff7f5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Interpretación de los Grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9caa8f7-c65c-4352-8c1b-85238f1df732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los centros de los clusters\n",
    "centers = best_model.stages[2].clusterCenters()\n",
    "print(\"Cluster Centers:\")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "# Graficar los clusters\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "# Aplicar PCA para reducir a 2 dimensiones\n",
    "pca = PCA(n_components=2)\n",
    "centers_2d = pca.fit_transform(centers)\n",
    "\n",
    "# Crear una figura\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Dibujar los puntos de los centros de los clusters en 2D\n",
    "plt.scatter(centers_2d[:, 0], centers_2d[:, 1], c='red', marker='x', s=100, label='Centers')\n",
    "\n",
    "# Puedes etiquetar los puntos con los números de los clusters\n",
    "for i, center in enumerate(centers):\n",
    "    plt.annotate(str(i), (centers_2d[i, 0], centers_2d[i, 1]), fontsize=12)\n",
    "\n",
    "plt.xlabel('PCA Dimension 1')\n",
    "plt.ylabel('PCA Dimension 2')\n",
    "plt.title('Centros de los Clusters en 2D')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d599db93-af81-4055-bfbe-6ab7031e2f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cluster_counts = predictions.groupBy(\"cluster\").count()\n",
    "# Muestra los resultados\n",
    "cluster_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a0e42f-a7f8-442d-a77d-345e51577da7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, round\n",
    "# Agrupa los datos por la columna de predicción\n",
    "grouped_data = predictions.groupBy(\"cluster\")\n",
    "\n",
    "# Calcula estadísticas descriptivas para cada grupo y característica\n",
    "mean_data = grouped_data.agg(\n",
    "    round(mean(\"Monto_Moneda_Local\"), 2).alias(\"Monto_Moneda_Local\"),\n",
    "    round(mean(\"Monto_Moneda_Extranjera\"), 2).alias(\"Monto_Moneda_Extranjera\"),\n",
    "    round(mean(\"Cuotas_Pagadas\"), 2).alias(\"Cuotas_Pagadas\"),\n",
    "    round(mean(\"Cuotas_Devengadas\"), 2).alias(\"Cuotas_Devengadas\"),\n",
    "    round(mean(\"Cuotas_Restantes\"), 2).alias(\"Cuotas_Restantes\"),\n",
    ").toPandas()\n",
    "\n",
    "mean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541dbec9-3af1-44eb-9e45-c0c89da987b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct,collect_set\n",
    "\n",
    "# Agrupa los datos por la columna \"cluster\" y cuenta valores únicos de \"aclaración\"\n",
    "count_data = predictions.groupBy(\"cluster\").agg(\n",
    "    countDistinct(\"aclaracion\").alias(\"count_aclaracion\"),\n",
    "    collect_set(\"aclaracion\").alias(\"unique_aclaracion_values\")\n",
    ")\n",
    "\n",
    "# Muestra los resultados\n",
    "count_data.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cluster_Caso_Real",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
